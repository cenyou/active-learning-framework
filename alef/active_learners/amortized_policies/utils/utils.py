# Copyright (c) 2024 Robert Bosch GmbH
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <https://www.gnu.org/licenses/>.

import torch
from torch.nn import Module, DataParallel
from linear_operator.operators import CatLinearOperator, DiagLinearOperator
from alef.configs.active_learners.amortized_policies import loss_configs

def wrap_parallel(module: Module, iscuda: bool, dim: int):
    if iscuda:
        return DataParallel(module, dim=dim).to(torch.device('cuda'))
    else:
        return module

def _unbind_data_for_different_kernels(kernel_list, x, mask=None, *, batch_dim: int=1):
    x_seq = x.unbind(batch_dim)
    assert len(x_seq) == len(kernel_list)
    if not mask is None:
        mask_seq = mask.unbind(batch_dim)
        assert len(mask_seq) == len(kernel_list)
        assert len(mask_seq[0].shape) == len(x_seq[0].shape) - 1
        for i, b in enumerate(mask_seq[0].shape[:-1]):
            assert b in [ 1, x_seq[0].shape[i] ]
        assert mask_seq[0].shape[-1] == x_seq[0].shape[-1]
    else:
        mask_seq = [None] * len(kernel_list)

    return x_seq, mask_seq

def compute_mean_batch(mean_list, x, mask=None, *, batch_dim: int=1):
    # X needs to be [*batch_size, D]
    # batch_dim[batch_dim]
    x_seq, mask_seq = _unbind_data_for_different_kernels(mean_list, x, mask=mask, batch_dim=batch_dim)

    M = []
    for i, mean_func in enumerate(mean_list):
        Mi = mean_func(x_seq[i], mask=mask_seq[i]).unsqueeze(batch_dim)
        M.append(Mi)

    return torch.concat(M, dim=batch_dim)

def compute_kernel_batch(kernel_list, x1, x2, noise_var_list=None, mask=None, *, batch_dim: int=1, return_linear_operator: bool=False):
    assert x1.device == x2.device
    device = x1.device

    x1_seq, mask_seq = _unbind_data_for_different_kernels(kernel_list, x1, mask=mask, batch_dim=batch_dim)
    x2_seq = x2.unbind(batch_dim)
    assert len(kernel_list) == len(x1_seq) == len(x2_seq)

    if not noise_var_list is None:
        assert len(kernel_list) == len(noise_var_list)
        assert x1.shape[-2] == x2.shape[-2]
        K = []
        for i, kernel in enumerate(kernel_list):
            Ki = kernel(x1_seq[i], x2_seq[i], mask=mask_seq[i]).unsqueeze(batch_dim)
            vi = DiagLinearOperator(noise_var_list[i] * torch.ones(x1.shape[-2], device=x1.device))
            K.append(Ki + vi)
    else:
        K = []
        for i, kernel in enumerate(kernel_list):
            Ki = kernel(x1_seq[i], x2_seq[i], mask=mask_seq[i]).unsqueeze(batch_dim)
            K.append(Ki)

    out = CatLinearOperator(*K, dim=batch_dim, output_device=device) if len(K) > 1 else K[0]
    if return_linear_operator:
        return out
    else:
        return out.to_dense()

def check_safety(loss_config):
    if isinstance(loss_config, loss_configs.BasicAmortizedPolicyLossConfig):
        return False
    elif isinstance(loss_config, loss_configs.BasicSafetyAwarePolicyLossConfig):
        return True
    elif isinstance(loss_config, loss_configs.BasicLossCurriculumConfig):
        return any([check_safety(lc) for lc in loss_config.loss_config_list])
    else:
        raise ValueError(f'Unknown loss config type: {type(loss_config)}')
