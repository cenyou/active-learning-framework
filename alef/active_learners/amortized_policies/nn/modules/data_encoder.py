# Copyright (c) 2024 Robert Bosch GmbH
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <https://www.gnu.org/licenses/>.

import numpy as np
import torch
import torch.nn as nn

"""
The following code is adapted from 
https://github.com/desi-ivanova/idad/blob/main/neural/modules.py
Copyright (c) 2021 Adam Foster, Desi R. Ivanova, Steven Kleinegesse, licensed under the MIT License,
cf. LICENSE file in the root directory of this source tree).
"""

class MLPDataEncoder(nn.Module):
    def __init__(
        self,
        input_dim, #[T, D], so [1, D]
        observation_dim, # [T, ], so 1
        hidden_dim,
        output_dim, # embedding dim so E
        activation=nn.ReLU(),
        output_activation=nn.Identity(),
        batch_norm=False,
        name=None,
    ):
        assert hasattr(hidden_dim, '__len__')
        super().__init__()
        self.input_dim = input_dim
        self.input_dim_flat = np.prod(input_dim) + observation_dim
        self.output_dim = output_dim
        self.output_dim_flat = np.prod(output_dim)
        self._name = name
        self.batch_norm = batch_norm

        ### Layers
        ### Layers
        if len(hidden_dim) == 0:
            self.linear1 = nn.Identity()
            self.bn1 = nn.Identity()
            self.activation = nn.Identity()
            self.middle = nn.Identity()
            self.output_layer = nn.Linear(self.input_dim_flat, self.output_dim_flat)
        else:
            self.linear1 = nn.Linear(self.input_dim_flat, hidden_dim[0])
            self.bn1 = (
                nn.BatchNorm1d(hidden_dim[0]) if self.batch_norm else nn.Identity()
            )
            self.activation = activation
            self.middle = nn.Sequential(
                *[
                    nn.Sequential(
                        nn.Linear(hidden_dim[i], hidden_dim[i + 1]),
                        nn.BatchNorm1d(hidden_dim[i + 1]) if self.batch_norm else nn.Identity(),
                        self.activation,
                    )
                    for i in range(0, len(hidden_dim) - 1)
                ]
            )
            self.output_layer = nn.Linear(hidden_dim[-1], self.output_dim_flat)

        self.output_activation = output_activation
        self.hidden_dim = hidden_dim

    def forward(self, x, y, **kwargs):
        inputs = torch.cat([x.flatten(-2), y], dim=-1)

        x = self.linear1(inputs)
        x = self.bn1(x)
        x = self.activation(x)
        x = self.middle(x)
        x = self.output_layer(x)
        x = self.output_activation(x)
        return x
